{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to PyTypes \u00b6 \u03bb poetry run python main.py --help Usage: main.py [OPTIONS] COMMAND [ARGS]... Options: --help Show this message and exit. Commands: confgen Generate pytypes.toml evaluate Evaluate given original and traced repository fetch Download repositories and apply tracing decorators typegen Generate type hinted files using trace data Workflow \u00b6 Fetching: poetry run python main.py fetch --help Confgen: poetry run python main.py confgen --help Tracing Typegen: poetry run python main.py typegen --help Evaluating: poetry run python main.py evaluate --help Miscellaneous \u00b6 Resolver Config Project layout \u00b6 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-pytypes","text":"\u03bb poetry run python main.py --help Usage: main.py [OPTIONS] COMMAND [ARGS]... Options: --help Show this message and exit. Commands: confgen Generate pytypes.toml evaluate Evaluate given original and traced repository fetch Download repositories and apply tracing decorators typegen Generate type hinted files using trace data","title":"Welcome to PyTypes"},{"location":"#workflow","text":"Fetching: poetry run python main.py fetch --help Confgen: poetry run python main.py confgen --help Tracing Typegen: poetry run python main.py typegen --help Evaluating: poetry run python main.py evaluate --help","title":"Workflow"},{"location":"#miscellaneous","text":"Resolver Config","title":"Miscellaneous"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"about/","text":"Project Motivation \u00b6","title":"About"},{"location":"about/#project-motivation","text":"","title":"Project Motivation"},{"location":"misc/config/","text":"Principles \u00b6 For this project's multi-stage workflow, some variables remain constant across multiple commands. For example, the paths necessary to differentiate between standard project types, standard library types and types from third-party dependencies in the virtualenv are needed for both the tracing and unification processes. All such variables are stored in a pytypes.toml file in the root of the project to trace, which will be read in by the program without the user having to respecify them. Example \u00b6 The following configuration is used for a project called PyTypes . The project is located in /home/name/repos/pytypes , the used Python binary's standard library is located at /usr/lib/python3.10 , and the virtual environment is located at /home/name/.cache/pypoetry/venv/pytypes-xvtnrWJT . To read up on why these paths are necessary, read up on the Resolver class and how types are stored into our trace data . Furthermore, customisable unifiers that are used during the annotation generation process are stored by a name and identified by the kind attribute. To read up on how these unifiers come into play, read up on the annotation generation process . [pytypes] project = \"PyTypes\" proj_path = \"/home/name/repos/pytypes\" stdlib_path = \"/usr/lib/python3.10\" venv_path = \"/home/name/.cache/pypoetry/venv/pytypes-xvtnrWJT\" [[unifier]] name = \"remove_dups\" kind = \"dedup\" [[unifier]] name = \"ignore_test\" kind = \"drop_test\" test_name_pat = \"test_\" [[unifier]] name = \"drop_implicit_2\" kind = \"drop_mult_var\" [[unifier]] name = \"drop_explicit_5\" kind = \"drop_mult_var\" min_amount_types_to_drop = 5 [[unifier]] name = \"unify_subtypes_relaxed\" kind = \"unify_subty\" [[unifier]] name = \"min_threshold\" kind = \"drop_min_threshold\" min_threshold = 0.3 [[unifier]] name = \"unify\" kind = \"union\"","title":"Configuration"},{"location":"misc/config/#principles","text":"For this project's multi-stage workflow, some variables remain constant across multiple commands. For example, the paths necessary to differentiate between standard project types, standard library types and types from third-party dependencies in the virtualenv are needed for both the tracing and unification processes. All such variables are stored in a pytypes.toml file in the root of the project to trace, which will be read in by the program without the user having to respecify them.","title":"Principles"},{"location":"misc/config/#example","text":"The following configuration is used for a project called PyTypes . The project is located in /home/name/repos/pytypes , the used Python binary's standard library is located at /usr/lib/python3.10 , and the virtual environment is located at /home/name/.cache/pypoetry/venv/pytypes-xvtnrWJT . To read up on why these paths are necessary, read up on the Resolver class and how types are stored into our trace data . Furthermore, customisable unifiers that are used during the annotation generation process are stored by a name and identified by the kind attribute. To read up on how these unifiers come into play, read up on the annotation generation process . [pytypes] project = \"PyTypes\" proj_path = \"/home/name/repos/pytypes\" stdlib_path = \"/usr/lib/python3.10\" venv_path = \"/home/name/.cache/pypoetry/venv/pytypes-xvtnrWJT\" [[unifier]] name = \"remove_dups\" kind = \"dedup\" [[unifier]] name = \"ignore_test\" kind = \"drop_test\" test_name_pat = \"test_\" [[unifier]] name = \"drop_implicit_2\" kind = \"drop_mult_var\" [[unifier]] name = \"drop_explicit_5\" kind = \"drop_mult_var\" min_amount_types_to_drop = 5 [[unifier]] name = \"unify_subtypes_relaxed\" kind = \"unify_subty\" [[unifier]] name = \"min_threshold\" kind = \"drop_min_threshold\" min_threshold = 0.3 [[unifier]] name = \"unify\" kind = \"union\"","title":"Example"},{"location":"misc/resolver/","text":"Principles \u00b6 The project has a need for a bidirectional lookup of types and modules; When the tracer implementation finds an instance that needs logging, it can query the Resolver for a module path and the instance's type's qualified name. Similary, when given strings containing a module path and a type's qualified name, the Resolver is capable of loading the requested type from its file. API \u00b6 Retrieving Module Path and Qualified Type Name from a type \u00b6 The process of type to module & path is performed by querying the given type using the __module__ and __file__ , together with sys.modules . sys.modules can be queried using a type 's __module__ attribute. For builtin types, this query delivers \"builtins\" , which is caught as an early-return. Using the [ __file__ ] attribute on the query's result, using the paths specified in the config file , the Resolver can determine by detection of relative paths whether the requested type is from the traced project, the standard library, or from a third-party dependency. Examples: \u00b6 >>> resolver . get_module_and_name ( ty = int ) ( None , 'int' ) >>> resolver . get_module_and_name ( ty = pathlib . Path ) ( 'pathlib' , 'Path' ) >>> resolver . get_module_and_name ( ty = fractions . Fraction ) ( 'fractions' , 'Fraction' ) >>> class Outer : ... class Inner : ... class EvenMoreInner : ... ... >>> r . get_module_and_name ( ty = Outer . Inner . EvenMoreInner ) ( '__main__' , 'Outer.Inner.EvenMoreInner' ) Creating a type from a Module Path and Qualified Type Name \u00b6 The process of module & path to type is facilitated by Python's importlib , which enables dynamic imports from modules. Examples: \u00b6 >>> resolver . type_lookup ( module_name = None , type_name = \"int\" ) < class ' int '> >>> resolver . type_lookup ( module_name = \"pathlib\" , type_name = \"Path\" ) < class ' pathlib . Path '> >>> resolver . type_lookup ( module_name = \"fractions\" , type_name = \"Fraction\" ) < class ' fractions . Fraction '> >>> class Outer : ... class Inner : ... class EvenMoreInner : ... ... >>> resolver . type_lookup ( ... module_name = \"__main__\" , ... type_name = \"Outer.Inner.EvenMoreInner\" ) < class ' __main__ . Outer . Inner . EvenMoreInner '> (The final example doesn't work in the REPL, because the repl cannot be passed as a project path, but tests show that it works in practice)","title":"Types, Modules & Qualified Names"},{"location":"misc/resolver/#principles","text":"The project has a need for a bidirectional lookup of types and modules; When the tracer implementation finds an instance that needs logging, it can query the Resolver for a module path and the instance's type's qualified name. Similary, when given strings containing a module path and a type's qualified name, the Resolver is capable of loading the requested type from its file.","title":"Principles"},{"location":"misc/resolver/#api","text":"","title":"API"},{"location":"misc/resolver/#retrieving-module-path-and-qualified-type-name-from-a-type","text":"The process of type to module & path is performed by querying the given type using the __module__ and __file__ , together with sys.modules . sys.modules can be queried using a type 's __module__ attribute. For builtin types, this query delivers \"builtins\" , which is caught as an early-return. Using the [ __file__ ] attribute on the query's result, using the paths specified in the config file , the Resolver can determine by detection of relative paths whether the requested type is from the traced project, the standard library, or from a third-party dependency.","title":"Retrieving Module Path and Qualified Type Name from a type"},{"location":"misc/resolver/#examples","text":">>> resolver . get_module_and_name ( ty = int ) ( None , 'int' ) >>> resolver . get_module_and_name ( ty = pathlib . Path ) ( 'pathlib' , 'Path' ) >>> resolver . get_module_and_name ( ty = fractions . Fraction ) ( 'fractions' , 'Fraction' ) >>> class Outer : ... class Inner : ... class EvenMoreInner : ... ... >>> r . get_module_and_name ( ty = Outer . Inner . EvenMoreInner ) ( '__main__' , 'Outer.Inner.EvenMoreInner' )","title":"Examples:"},{"location":"misc/resolver/#creating-a-type-from-a-module-path-and-qualified-type-name","text":"The process of module & path to type is facilitated by Python's importlib , which enables dynamic imports from modules.","title":"Creating a type from a Module Path and Qualified Type Name"},{"location":"misc/resolver/#examples_1","text":">>> resolver . type_lookup ( module_name = None , type_name = \"int\" ) < class ' int '> >>> resolver . type_lookup ( module_name = \"pathlib\" , type_name = \"Path\" ) < class ' pathlib . Path '> >>> resolver . type_lookup ( module_name = \"fractions\" , type_name = \"Fraction\" ) < class ' fractions . Fraction '> >>> class Outer : ... class Inner : ... class EvenMoreInner : ... ... >>> resolver . type_lookup ( ... module_name = \"__main__\" , ... type_name = \"Outer.Inner.EvenMoreInner\" ) < class ' __main__ . Outer . Inner . EvenMoreInner '> (The final example doesn't work in the REPL, because the repl cannot be passed as a project path, but tests show that it works in practice)","title":"Examples:"},{"location":"workflow/annotating/","text":"Functionality \u00b6 The typegen module contains classes & functions to unify trace data and generate files with type hints. It also offers a command to execute the typegen workflow. \u03bb poetry run python main.py typegen --help Usage: main.py typegen [OPTIONS] Generate type hinted files using trace data Options: -p, --path PATH Path to project directory [required] -u, --unifiers TEXT Unifier to apply, as given by `name` in pytypes.toml under [[unifier]] -g, --gen-strat [stub|inline|eval_inline] Select a strategy for generating type hints [required] -v, --verbose INFO if not given, else DEBUG --help Show this message and exit. Example usage: \u03bb poetry run python main.py typegen \\ -p project_path -g eval_inline \\ -u mt -u dupl -u mult2 -u dupl -u first Foundations & Principles \u00b6 The project's approach to annotating is CST-based (Concrete-Syntax-Tree) transformation. A CST differs from an AST in that it keeps all formatting details, including comments, whitespace and parantheses, and is therefore a fitting choice for the project, as only minimals modifications to a repository's code should occur, i.e. annotations and decorators for tracing. In regards thereto, Python's AST module is a poor choice, as it only contains elements that are necessary for code execution, i.e. it drops extraneous newlines, changes all quote signs to single quotes, and perhaps worst of all, it removes all single-line comments. The project offers both inline and stub-based annotation generation, whose mechanics, both shared and individual, have been split into CST transformers. By reading in files that have been traced, trace data can be segmented on a per-file basis. From this per-file basis trace data, annotations (also called type hints) can be generated for each file using the aforementioned transformers, and output appropriately. Unification \u00b6 The trace data must be cleaned and appropriately unified to remove redundant data so that at most one type hint can be associated with each traced instance. To this extent, unifiers with a common interface have been implemented to cover unification needs. The unifiers are applied to the trace data in the order specified on the command line; the value given to each -u flag references a unifier by the name key given in the config file . The examples in the following section only contain \"Category\", \"VarName\", \"TypeModule\" and \"Type\" for brevity's sake. Unifiers \u00b6 Every unifier performs a different operation upon the trace data. These can largely be segmented into two categories, namely 'filtering' and 'reducing'. The former removes occurrences of trace data that are undesirable, and the latter groups rows and make replacements where appropriate. Filtering Unifiers \u00b6 Drop Duplicates \u00b6 While the tracer implementation may deduplicate trace data after halting, when the trace data is loaded into memory, every test that shares a call-path usually holds the same information, which is redundant, and can therefore be removed. Example: print ( \"Hello World\" ) Drop Test Functions \u00b6 The project always applies its decorators to test functions and methods , which leads to trace information about the test also being stored, and possibly generated during the annotation process . If the user does not want to retain this information, then this filter should be applied, which simply drops all rows that reference testing callables. Example: Min-Threshold \u00b6 Drop all rows whose types appear less often than the minimum threshold. This is a simple attempt to detect API misusage in tests; if a statistically significant amount of tests use a certain signature, and a very low amount of other tests use a different one, then this unifier will remove those rows. Example: Reducing Unifiers \u00b6 Subtypes & Common Interfaces \u00b6 Replaces rows containing types of the same variable in the data with their earliest common base type. This unifier uses the Resolver implementation to load the MRO s for such rows in order to find the said shared base type. The instance can also be defined so that only type hints are replaced if the common base type is also in the trace data. No replacement occurs for undesirable base types, such as abc.ABC , abc.ABCMeta and object . Example: Unions \u00b6 Replaces rows containing types of the same variable in the data with the union of these types. Example: Transformers \u00b6 To apply changes to the files / add type hints to the code, the code in the file is parsed into a CST. Modifying the CSTs is done by transformers which visit the nodes in the trees and modify these if necessary. The following transformers are used to achieve the functionality of the type hint generators: TypeHintTransformer - Updates aug-/assign, function definition & function parameter nodes by adding the traced type hints to the corresponding variables if that variable is in the trace data. If an already existing type hint exists, the node will not be changed. Used to add traced type hints to global, local variables and class members in assignments, to function parameters and function return in function definitions. RemoveAllTypeHintsTransformer - Removes type hints from annotated assign, function definition & function parameter nodes. Used by the evaluation inline generator to remove existing type hints of global, local variables and class members in assignments, to function parameters and function return in function definitions. AddImportTransformer - Transforms the CST by adding Import-From nodes to import the modules of the type hints in the trace data. Used to update the code in the files so that the modules of the added type hints are imported. MyPyHintTransformer - Uses the given CST to generate the corresponding stub CST. This is done by saving the CST code in a temporary file, generating the corresponding stub file (also as a temporary file) using mypy.stubgen and parsing the stub file's contents into the stub CST. Used by the stub file generator to generate the stub CST after adding the traced type hints to the CST. ImportUnionTransformer - Transforms the CST by adding the Import-From node to import Union from typing ( from typing import Union ) if the corresponding code contains a type hint which uses Union . Used by the stub file generator to add the missing import in the stub CST as mypy.stubgen annotates with Union , but does not add the corresponding import. Type Hint Generators \u00b6 After unifying the trace data, a type hint generator is used to generate files with. Which type hint generator is used is specified by the --gen-strat option. Type Hint Generators are instances which generate the files with traced type hints for callables and variables using the filtered trace data. As the trace data contains the filenames, The constraint of the trace data is that to each variable, only one type hint exists. InlineGenerator - Overwrites the files by adding the traced type hints inline. Does not overwrite existing type hints. Uses the TypeHintTransformer followed by the AddImportTransformer . EvaluationInlineGenerator - Overwrites the files by adding the inline type hints, and removing annotations for instances that do not have any trace data. Used to evaluate the traced type hints compared with the existing type hints . Uses the RemoveAllTypeHintsTransformer , followed by the TypeHintTransformer and AddImportTransformer . StubFileGenerator - Generates .pyi stub files of the affected files with the traced type hints. Existing type hints are kept. Uses the TypeHintTransformer followed by the AddImportTransformer , MyPyHintTransformer and ImportUnionTransformer , in that order. Developer Documentation \u00b6 Drop Test Functions - Drops all data about test functions. Used to remove data about test functions to prevent test functions to be annotated by the type hint generator . Drop of multiple types - Drops rows containing variables of multiple types. Min-Threshold - Drops all rows whose types appear less often than the minimum threshold. Keep only first - Keeps only the first row of each variable. Used to ensure that each variable in the trace data has only one type hint. Often used as the last filter. Unify subtypes - Replaces rows containing types of the same variable in the data with their common base type. Does not replace if the common base type is ABC , ABCMeta or object . Used to unify the traced type hints of the variables. The instance can also be defined so that only type hints are replaced if the common base type is also in the trace data. Union - Replaces rows containing types of the same variable in the data with the union of these types. Used to unify the traced type hints of the variables. Filter List - Applies the filters in this list on the trace data. Used to filter the trace data with multiple filters, one-by-one. This is possible due to the common base class","title":"Annotating"},{"location":"workflow/annotating/#functionality","text":"The typegen module contains classes & functions to unify trace data and generate files with type hints. It also offers a command to execute the typegen workflow. \u03bb poetry run python main.py typegen --help Usage: main.py typegen [OPTIONS] Generate type hinted files using trace data Options: -p, --path PATH Path to project directory [required] -u, --unifiers TEXT Unifier to apply, as given by `name` in pytypes.toml under [[unifier]] -g, --gen-strat [stub|inline|eval_inline] Select a strategy for generating type hints [required] -v, --verbose INFO if not given, else DEBUG --help Show this message and exit. Example usage: \u03bb poetry run python main.py typegen \\ -p project_path -g eval_inline \\ -u mt -u dupl -u mult2 -u dupl -u first","title":"Functionality"},{"location":"workflow/annotating/#foundations-principles","text":"The project's approach to annotating is CST-based (Concrete-Syntax-Tree) transformation. A CST differs from an AST in that it keeps all formatting details, including comments, whitespace and parantheses, and is therefore a fitting choice for the project, as only minimals modifications to a repository's code should occur, i.e. annotations and decorators for tracing. In regards thereto, Python's AST module is a poor choice, as it only contains elements that are necessary for code execution, i.e. it drops extraneous newlines, changes all quote signs to single quotes, and perhaps worst of all, it removes all single-line comments. The project offers both inline and stub-based annotation generation, whose mechanics, both shared and individual, have been split into CST transformers. By reading in files that have been traced, trace data can be segmented on a per-file basis. From this per-file basis trace data, annotations (also called type hints) can be generated for each file using the aforementioned transformers, and output appropriately.","title":"Foundations &amp; Principles"},{"location":"workflow/annotating/#unification","text":"The trace data must be cleaned and appropriately unified to remove redundant data so that at most one type hint can be associated with each traced instance. To this extent, unifiers with a common interface have been implemented to cover unification needs. The unifiers are applied to the trace data in the order specified on the command line; the value given to each -u flag references a unifier by the name key given in the config file . The examples in the following section only contain \"Category\", \"VarName\", \"TypeModule\" and \"Type\" for brevity's sake.","title":"Unification"},{"location":"workflow/annotating/#unifiers","text":"Every unifier performs a different operation upon the trace data. These can largely be segmented into two categories, namely 'filtering' and 'reducing'. The former removes occurrences of trace data that are undesirable, and the latter groups rows and make replacements where appropriate.","title":"Unifiers"},{"location":"workflow/annotating/#filtering-unifiers","text":"","title":"Filtering Unifiers"},{"location":"workflow/annotating/#drop-duplicates","text":"While the tracer implementation may deduplicate trace data after halting, when the trace data is loaded into memory, every test that shares a call-path usually holds the same information, which is redundant, and can therefore be removed. Example: print ( \"Hello World\" )","title":"Drop Duplicates"},{"location":"workflow/annotating/#drop-test-functions","text":"The project always applies its decorators to test functions and methods , which leads to trace information about the test also being stored, and possibly generated during the annotation process . If the user does not want to retain this information, then this filter should be applied, which simply drops all rows that reference testing callables. Example:","title":"Drop Test Functions"},{"location":"workflow/annotating/#min-threshold","text":"Drop all rows whose types appear less often than the minimum threshold. This is a simple attempt to detect API misusage in tests; if a statistically significant amount of tests use a certain signature, and a very low amount of other tests use a different one, then this unifier will remove those rows. Example:","title":"Min-Threshold"},{"location":"workflow/annotating/#reducing-unifiers","text":"","title":"Reducing Unifiers"},{"location":"workflow/annotating/#subtypes-common-interfaces","text":"Replaces rows containing types of the same variable in the data with their earliest common base type. This unifier uses the Resolver implementation to load the MRO s for such rows in order to find the said shared base type. The instance can also be defined so that only type hints are replaced if the common base type is also in the trace data. No replacement occurs for undesirable base types, such as abc.ABC , abc.ABCMeta and object . Example:","title":"Subtypes &amp; Common Interfaces"},{"location":"workflow/annotating/#unions","text":"Replaces rows containing types of the same variable in the data with the union of these types. Example:","title":"Unions"},{"location":"workflow/annotating/#transformers","text":"To apply changes to the files / add type hints to the code, the code in the file is parsed into a CST. Modifying the CSTs is done by transformers which visit the nodes in the trees and modify these if necessary. The following transformers are used to achieve the functionality of the type hint generators: TypeHintTransformer - Updates aug-/assign, function definition & function parameter nodes by adding the traced type hints to the corresponding variables if that variable is in the trace data. If an already existing type hint exists, the node will not be changed. Used to add traced type hints to global, local variables and class members in assignments, to function parameters and function return in function definitions. RemoveAllTypeHintsTransformer - Removes type hints from annotated assign, function definition & function parameter nodes. Used by the evaluation inline generator to remove existing type hints of global, local variables and class members in assignments, to function parameters and function return in function definitions. AddImportTransformer - Transforms the CST by adding Import-From nodes to import the modules of the type hints in the trace data. Used to update the code in the files so that the modules of the added type hints are imported. MyPyHintTransformer - Uses the given CST to generate the corresponding stub CST. This is done by saving the CST code in a temporary file, generating the corresponding stub file (also as a temporary file) using mypy.stubgen and parsing the stub file's contents into the stub CST. Used by the stub file generator to generate the stub CST after adding the traced type hints to the CST. ImportUnionTransformer - Transforms the CST by adding the Import-From node to import Union from typing ( from typing import Union ) if the corresponding code contains a type hint which uses Union . Used by the stub file generator to add the missing import in the stub CST as mypy.stubgen annotates with Union , but does not add the corresponding import.","title":"Transformers"},{"location":"workflow/annotating/#type-hint-generators","text":"After unifying the trace data, a type hint generator is used to generate files with. Which type hint generator is used is specified by the --gen-strat option. Type Hint Generators are instances which generate the files with traced type hints for callables and variables using the filtered trace data. As the trace data contains the filenames, The constraint of the trace data is that to each variable, only one type hint exists. InlineGenerator - Overwrites the files by adding the traced type hints inline. Does not overwrite existing type hints. Uses the TypeHintTransformer followed by the AddImportTransformer . EvaluationInlineGenerator - Overwrites the files by adding the inline type hints, and removing annotations for instances that do not have any trace data. Used to evaluate the traced type hints compared with the existing type hints . Uses the RemoveAllTypeHintsTransformer , followed by the TypeHintTransformer and AddImportTransformer . StubFileGenerator - Generates .pyi stub files of the affected files with the traced type hints. Existing type hints are kept. Uses the TypeHintTransformer followed by the AddImportTransformer , MyPyHintTransformer and ImportUnionTransformer , in that order.","title":"Type Hint Generators"},{"location":"workflow/annotating/#developer-documentation","text":"Drop Test Functions - Drops all data about test functions. Used to remove data about test functions to prevent test functions to be annotated by the type hint generator . Drop of multiple types - Drops rows containing variables of multiple types. Min-Threshold - Drops all rows whose types appear less often than the minimum threshold. Keep only first - Keeps only the first row of each variable. Used to ensure that each variable in the trace data has only one type hint. Often used as the last filter. Unify subtypes - Replaces rows containing types of the same variable in the data with their common base type. Does not replace if the common base type is ABC , ABCMeta or object . Used to unify the traced type hints of the variables. The instance can also be defined so that only type hints are replaced if the common base type is also in the trace data. Union - Replaces rows containing types of the same variable in the data with the union of these types. Used to unify the traced type hints of the variables. Filter List - Applies the filters in this list on the trace data. Used to filter the trace data with multiple filters, one-by-one. This is possible due to the common base class","title":"Developer Documentation"},{"location":"workflow/evaluating/","text":"Functionality \u00b6 The evaluation module contains classes/functions to evaluate the traced type hints by using the previously existing type hints. A requirement for successful/meaningful evaluation is the usage of the evaluation inline generator when annotating the traced type hints to the files. Additionally, also contains classes/functions to evaluate the speed of the tracing compared and without tracing. \u03bb poetry run python main.py evaluate --help Usage: main.py evaluate [OPTIONS] Evaluate given original and traced repository Options: -o, --original PATH Path to original project directory [required] -t, --traced PATH Path to traced project directory [required] -s, --store PATH Path to store performance & metric data [required] -d, --data_name TEXT Name for data files --help Show this message and exit. Example usage: \u03bb poetry run python main.py evaluate \\ -o original_project_path \\ -t traced_project_path \\ -s path_to_save_evaluation_data \\ -d data_file_name Note: The command does not evaluate the repositories, it stores the data necessary for evaluation. The actual evaluation can be done by loading the data and analyzing it. The template file ipynb_evaluation_template.py in evaluation can be used as a template / base implementation for the jupyter notebook file to evaluate the data. On executing the command, the following steps are done: Foundations & Principles \u00b6 The project's approach to evaluation is to compare the repository/project before, and after annotating the type hints from tracing (will be called original repository and traced repository). The goal is find out which variables still keep the type hints, and which have different type hints after tracing. By comparing the type hints, the quality of the tracing can be measured. Additionally, potential issues of the tracing and traced type hint annotation can be identified and the project can be improved. The data about the speed of the tracing compared and without tracing can also be evaluated. For more details, see Performance Data . Typehint data \u00b6 When comparing a file before and after traced type hint annotation, the type hints have to be compared. This is done by comparing the so-called typehint data of the original and the traced file (= the original file after annotating). The typehint data contains the information about type hints of a file/multiple files. It is used to find out which variables have what kind of type hint. By comparing the typehint data of the original and traced file, it can be determined whether the type hints of the matching variables also match or differ. Column Meaning Type Null? Filename Relative path to file of traced instance from project root string Never Class Name of class traced instance is in string When not in a class' scope FunctionName Name of function traced instance is in string When not in a function's scope ColumnOffset The column offset of the line the traced instance occurs on uint Never Category Number identifying context traced instance appears in int Never VarName Name of traced instance string Never Type Name of traced instance's type string Never As the line numbers of the original and traced file due to the additional imports differ, the column offset is used instead. It corresponds to the scope of the variable. To determine the typehint data for one or more files, the FileTypeHintsCollector is used. Metric data \u00b6 The comparison of the original and traced typehint data is done by merging their rows. This is done to find out which variables in the original and traced typehint data are matching. Due to the merge, two type columns exist: The type in the original typehint data and the traced typehint data. Additionally, it is checked whether a variable which has a type hint in one file also has a type hint in the other file. The corresponding information cna be found in the Completeness column. It returns the information whether the same variable in the original and in the traced file has a type hint in both files. It is null if there is a type hint in the traced file, but not in the original. In addition to that, the Correctness Column is introduced which returns the information whether the type hints match. It is automatically False, if the corresponding completeness is False and null if the corresponding completeness is null. The merged data which the new schema is called \"metric data\". Column Meaning Type Null? Filename Relative path to file of traced instance from project root string Never Class Name of class traced instance is in string When not in a class' scope FunctionName Name of function traced instance is in string When not in a function's scope ColumnOffset The column offset of the line the traced instance occurs on uint Never Category Number identifying context traced instance appears in int Never VarName Name of traced instance string Never OriginalType Name of the type hint in the original files string When not existing in the original typehint data GeneratedType Name of the type hint in the traced files string When not existing in the traced typehint data Completeness Do OriginalType and GeneratedType exist? bool When OriginalType is null Correctness Do OriginalType and GeneratedType match? bool When OriginalType is null Apart from determining the total completeness and correctness, it can also be used to determine the completeness and correctness for each file. It can be evaluated which files have high completeness and/or high correctness or which don't. With this, the files with low completeness/low correctness can be traced and figure out the issues. Thus, improvements can be figured out to improve the quality of the tracing and traced type hint annotation . To determine the metric data given two typehint data instances, the metric data calculator is used. Performance data \u00b6 Apart from generating the trace data, the tracing can also generate the so-called performance data. This can be done by setting the benchmark_performance value to True in the configuration . It contains the execution times of the test function without tracing, with tracing without optimizations and with optimizations. Additionally, the tracing is also benchmarked by the the minimum implementation of a tracer (The TracerBase/the NoOperationTracer). It can be used to evaluate whether the tracer is faster with/without optimizations and how much slower it is compared to execution without tracing. Compared to other data schemas, the times are stored in an array ( np.ndarray ). Collecting and deserializing the performance data is done by the PerformanceDataFileCollector Trace Data File Collection & Deserialization \u00b6 The trace data files which have been generated by tracing have to be collected and deserialized into one single dataframe. This is done by using the trace data file collector . This is used to find out which files have been changed by traced type hint annotation . Original file paths and traced file paths collection \u00b6 The file paths in the trace data are iterated. Combined with the original repository & traced repository path, the original & traced file paths are determined. The corresponding files are compared. If the traced type hint annotation did not change the file, then the file contents of the original file and the file after annotation are the same. If the file contents are different, then the annotating has modified the file. The resulting original and traced file paths whose corresponding files differ are used to determine the original typehint data and the traced typehint data. Typehint data collection \u00b6 To collect the typehint data for multiple files, the FileTypeHintsCollector is used. The instance collects the typehint data for each file and returns the data as a single dataframe. This is done for the original file paths and the traced file paths, resulting in 2 typehint data instances: The original and traced typehint data. Metric data calculation \u00b6 After getting the original and traced typehint data, the metric data calculator is used to get the metric data. Saving the metric and performance data \u00b6 After getting the metric data, it is serialized in the data file path provided by the command options. Additionally, the performance data is collected and deserialized by the PerformanceDataFileCollector into one single array. The array is also serialized in the same data file path; only with a different file extension. After executing the command, the metric and performance data are stored in the data file paths. A jupyter notebook can be used to analyze the data. The template file ipynb_evaluation_template.py in evaluation can be used as a template/base implementation for the jupyter notebook file to evaluate the data. API \u00b6 You can find the usage of the classes/its instances by checking their corresponding tests. FileTypeHintsCollector \u00b6 Given a folder path/ or one or more file paths, collects the type hints in the .py files and stores these in a typehint data instance. It does this by parsing the code of each file to a CST (common syntax tree) and finding the type hints of each variable/function return.with a visitor. To ensure that matching types are stored in the typehint data with the same type name, multiple normalization algorithms are used to include the modules to the name and unify type unions. Examples: from pathlib import Path import pathlib import typing a : Path = ... # -> Collected type hint name is pathlib.Path b : pathlib . Path = ... # -> Collected type hint name is pathlib.Path c : str | int = ... # -> Collected type hint name is int | str d : typing . Union [ int , str ] = ... # -> Collected type hint name is int | str Used to get the typehint data of multiple files. Note: Using an AST would probably also work, but since CSTs have already been used in the typegen module, the same library has been used. MetricDataCalculator \u00b6 Given two typehint data instances (considered as the original and traced typehint data), calculates the corresponding metric data. Is done by merging the typehint data instances. Note: Due to using the column offsets instead of the line numbers, following conflicts can arise: Original file: e : bool = True e : str = \"str\" Traced file: e = True e : bool = \"str\" or: e : bool = True e = \"str\" The original typehint data contains two rows for the two lines in which 'e' is assigned to a value. The contents of the two rows, except for the type name, match. Compared to the original typehint data, the traced typehint data only has one row in both cases. Since the line number does not exist and the column offset is the same in both cases, the traced typehint data would be the same in both cases. Thus, it is not possible to differentiate between the two cases, resulting in the following conflict: When merging, it has to be decided whether the rows containing the information for e: bool (Type hints match) should be merged or whether e: str of the original typehint data and e: bool of the traced typehint data should be merged (type hints differ). This can affect the completeness and correctness. The metric data calculator is defined in such a way that it does the former, increasing the correctness. PerformanceDataFileCollector \u00b6 Given a folder path, collects trace data files and deserializes them into one single performance data dataframe. Bonus: Evaluation Template \u00b6 Not part of the API as it is considered a jupyter notebook file. To load the template .py file in jupyter notebook, install the jupytext module. Loads the metric & performance data and determines the total completeness and correctness. Also determines which files have a high/low completeness/correctness. Additionally, analyzes the performance data by plotting the execution times with tracing compared to times without tracing as scatter points. Can be used as a template/base implementation for evaluating the metric & performance data.","title":"Evaluating"},{"location":"workflow/evaluating/#functionality","text":"The evaluation module contains classes/functions to evaluate the traced type hints by using the previously existing type hints. A requirement for successful/meaningful evaluation is the usage of the evaluation inline generator when annotating the traced type hints to the files. Additionally, also contains classes/functions to evaluate the speed of the tracing compared and without tracing. \u03bb poetry run python main.py evaluate --help Usage: main.py evaluate [OPTIONS] Evaluate given original and traced repository Options: -o, --original PATH Path to original project directory [required] -t, --traced PATH Path to traced project directory [required] -s, --store PATH Path to store performance & metric data [required] -d, --data_name TEXT Name for data files --help Show this message and exit. Example usage: \u03bb poetry run python main.py evaluate \\ -o original_project_path \\ -t traced_project_path \\ -s path_to_save_evaluation_data \\ -d data_file_name Note: The command does not evaluate the repositories, it stores the data necessary for evaluation. The actual evaluation can be done by loading the data and analyzing it. The template file ipynb_evaluation_template.py in evaluation can be used as a template / base implementation for the jupyter notebook file to evaluate the data. On executing the command, the following steps are done:","title":"Functionality"},{"location":"workflow/evaluating/#foundations-principles","text":"The project's approach to evaluation is to compare the repository/project before, and after annotating the type hints from tracing (will be called original repository and traced repository). The goal is find out which variables still keep the type hints, and which have different type hints after tracing. By comparing the type hints, the quality of the tracing can be measured. Additionally, potential issues of the tracing and traced type hint annotation can be identified and the project can be improved. The data about the speed of the tracing compared and without tracing can also be evaluated. For more details, see Performance Data .","title":"Foundations &amp; Principles"},{"location":"workflow/evaluating/#typehint-data","text":"When comparing a file before and after traced type hint annotation, the type hints have to be compared. This is done by comparing the so-called typehint data of the original and the traced file (= the original file after annotating). The typehint data contains the information about type hints of a file/multiple files. It is used to find out which variables have what kind of type hint. By comparing the typehint data of the original and traced file, it can be determined whether the type hints of the matching variables also match or differ. Column Meaning Type Null? Filename Relative path to file of traced instance from project root string Never Class Name of class traced instance is in string When not in a class' scope FunctionName Name of function traced instance is in string When not in a function's scope ColumnOffset The column offset of the line the traced instance occurs on uint Never Category Number identifying context traced instance appears in int Never VarName Name of traced instance string Never Type Name of traced instance's type string Never As the line numbers of the original and traced file due to the additional imports differ, the column offset is used instead. It corresponds to the scope of the variable. To determine the typehint data for one or more files, the FileTypeHintsCollector is used.","title":"Typehint data"},{"location":"workflow/evaluating/#metric-data","text":"The comparison of the original and traced typehint data is done by merging their rows. This is done to find out which variables in the original and traced typehint data are matching. Due to the merge, two type columns exist: The type in the original typehint data and the traced typehint data. Additionally, it is checked whether a variable which has a type hint in one file also has a type hint in the other file. The corresponding information cna be found in the Completeness column. It returns the information whether the same variable in the original and in the traced file has a type hint in both files. It is null if there is a type hint in the traced file, but not in the original. In addition to that, the Correctness Column is introduced which returns the information whether the type hints match. It is automatically False, if the corresponding completeness is False and null if the corresponding completeness is null. The merged data which the new schema is called \"metric data\". Column Meaning Type Null? Filename Relative path to file of traced instance from project root string Never Class Name of class traced instance is in string When not in a class' scope FunctionName Name of function traced instance is in string When not in a function's scope ColumnOffset The column offset of the line the traced instance occurs on uint Never Category Number identifying context traced instance appears in int Never VarName Name of traced instance string Never OriginalType Name of the type hint in the original files string When not existing in the original typehint data GeneratedType Name of the type hint in the traced files string When not existing in the traced typehint data Completeness Do OriginalType and GeneratedType exist? bool When OriginalType is null Correctness Do OriginalType and GeneratedType match? bool When OriginalType is null Apart from determining the total completeness and correctness, it can also be used to determine the completeness and correctness for each file. It can be evaluated which files have high completeness and/or high correctness or which don't. With this, the files with low completeness/low correctness can be traced and figure out the issues. Thus, improvements can be figured out to improve the quality of the tracing and traced type hint annotation . To determine the metric data given two typehint data instances, the metric data calculator is used.","title":"Metric data"},{"location":"workflow/evaluating/#performance-data","text":"Apart from generating the trace data, the tracing can also generate the so-called performance data. This can be done by setting the benchmark_performance value to True in the configuration . It contains the execution times of the test function without tracing, with tracing without optimizations and with optimizations. Additionally, the tracing is also benchmarked by the the minimum implementation of a tracer (The TracerBase/the NoOperationTracer). It can be used to evaluate whether the tracer is faster with/without optimizations and how much slower it is compared to execution without tracing. Compared to other data schemas, the times are stored in an array ( np.ndarray ). Collecting and deserializing the performance data is done by the PerformanceDataFileCollector","title":"Performance data"},{"location":"workflow/evaluating/#trace-data-file-collection-deserialization","text":"The trace data files which have been generated by tracing have to be collected and deserialized into one single dataframe. This is done by using the trace data file collector . This is used to find out which files have been changed by traced type hint annotation .","title":"Trace Data File Collection &amp; Deserialization"},{"location":"workflow/evaluating/#original-file-paths-and-traced-file-paths-collection","text":"The file paths in the trace data are iterated. Combined with the original repository & traced repository path, the original & traced file paths are determined. The corresponding files are compared. If the traced type hint annotation did not change the file, then the file contents of the original file and the file after annotation are the same. If the file contents are different, then the annotating has modified the file. The resulting original and traced file paths whose corresponding files differ are used to determine the original typehint data and the traced typehint data.","title":"Original file paths and traced file paths collection"},{"location":"workflow/evaluating/#typehint-data-collection","text":"To collect the typehint data for multiple files, the FileTypeHintsCollector is used. The instance collects the typehint data for each file and returns the data as a single dataframe. This is done for the original file paths and the traced file paths, resulting in 2 typehint data instances: The original and traced typehint data.","title":"Typehint data collection"},{"location":"workflow/evaluating/#metric-data-calculation","text":"After getting the original and traced typehint data, the metric data calculator is used to get the metric data.","title":"Metric data calculation"},{"location":"workflow/evaluating/#saving-the-metric-and-performance-data","text":"After getting the metric data, it is serialized in the data file path provided by the command options. Additionally, the performance data is collected and deserialized by the PerformanceDataFileCollector into one single array. The array is also serialized in the same data file path; only with a different file extension. After executing the command, the metric and performance data are stored in the data file paths. A jupyter notebook can be used to analyze the data. The template file ipynb_evaluation_template.py in evaluation can be used as a template/base implementation for the jupyter notebook file to evaluate the data.","title":"Saving the metric and performance data"},{"location":"workflow/evaluating/#api","text":"You can find the usage of the classes/its instances by checking their corresponding tests.","title":"API"},{"location":"workflow/evaluating/#filetypehintscollector","text":"Given a folder path/ or one or more file paths, collects the type hints in the .py files and stores these in a typehint data instance. It does this by parsing the code of each file to a CST (common syntax tree) and finding the type hints of each variable/function return.with a visitor. To ensure that matching types are stored in the typehint data with the same type name, multiple normalization algorithms are used to include the modules to the name and unify type unions. Examples: from pathlib import Path import pathlib import typing a : Path = ... # -> Collected type hint name is pathlib.Path b : pathlib . Path = ... # -> Collected type hint name is pathlib.Path c : str | int = ... # -> Collected type hint name is int | str d : typing . Union [ int , str ] = ... # -> Collected type hint name is int | str Used to get the typehint data of multiple files. Note: Using an AST would probably also work, but since CSTs have already been used in the typegen module, the same library has been used.","title":"FileTypeHintsCollector"},{"location":"workflow/evaluating/#metricdatacalculator","text":"Given two typehint data instances (considered as the original and traced typehint data), calculates the corresponding metric data. Is done by merging the typehint data instances. Note: Due to using the column offsets instead of the line numbers, following conflicts can arise: Original file: e : bool = True e : str = \"str\" Traced file: e = True e : bool = \"str\" or: e : bool = True e = \"str\" The original typehint data contains two rows for the two lines in which 'e' is assigned to a value. The contents of the two rows, except for the type name, match. Compared to the original typehint data, the traced typehint data only has one row in both cases. Since the line number does not exist and the column offset is the same in both cases, the traced typehint data would be the same in both cases. Thus, it is not possible to differentiate between the two cases, resulting in the following conflict: When merging, it has to be decided whether the rows containing the information for e: bool (Type hints match) should be merged or whether e: str of the original typehint data and e: bool of the traced typehint data should be merged (type hints differ). This can affect the completeness and correctness. The metric data calculator is defined in such a way that it does the former, increasing the correctness.","title":"MetricDataCalculator"},{"location":"workflow/evaluating/#performancedatafilecollector","text":"Given a folder path, collects trace data files and deserializes them into one single performance data dataframe.","title":"PerformanceDataFileCollector"},{"location":"workflow/evaluating/#bonus-evaluation-template","text":"Not part of the API as it is considered a jupyter notebook file. To load the template .py file in jupyter notebook, install the jupytext module. Loads the metric & performance data and determines the total completeness and correctness. Also determines which files have a high/low completeness/correctness. Additionally, analyzes the performance data by plotting the execution times with tracing compared to times without tracing as scatter points. Can be used as a template/base implementation for evaluating the metric & performance data.","title":"Bonus: Evaluation Template"},{"location":"workflow/fetching/","text":"Foundations & Principles \u00b6 Modify given codebase by detecting test folders and applying decorator for tracing to test functions Supported Formats \u00b6 Git repository Local folder Archives Decorator Application \u00b6","title":"Fetching"},{"location":"workflow/fetching/#foundations-principles","text":"Modify given codebase by detecting test folders and applying decorator for tracing to test functions","title":"Foundations &amp; Principles"},{"location":"workflow/fetching/#supported-formats","text":"Git repository Local folder Archives","title":"Supported Formats"},{"location":"workflow/fetching/#decorator-application","text":"","title":"Decorator Application"},{"location":"workflow/tracing/","text":"Functionality \u00b6 The tracing module contains classes that facilitate the tracing process behind a user-friendly API. Note that it does not offer a command for performing the tracing procedure, which should instead be done by executing the decorated tests of the project . The testing process can be highly customised on a per-project basis, and as such represents a high-effort low-reward coding investment on this project's end. Foundations & Principles \u00b6 sys.settrace \u00b6 sys.settrace is a Python function that allows a callable to be set that is invoked on every line of Python code that comes after it. This registered callable, henceforth refered to as the trace function is expected to have three arguments: frame : a representation of the current stack frame, containing references to further execution-related objects, such as: the previous frame visible globals variables that have been placed on the stack and more event : a string indicating the manner in which the current line of Python is handled. Relevant for us are: call: a callable was entered line: plain line of code that is about to be executed (NOTE: this means the line will executed in the next interpreter step, not when it is encountered by the trace function) return: a callable is about to return arg : a value that differs depending on the given event. Relevant for us are: call: arg is None. Retrieving the values of arguments is to be performed separately. line: arg is None. Retrieving the values of variables on this line is also to be performed separately. return: arg is the value that will be returned from the callable. Effect on Coverage, Debugging and other trace-related Tooling \u00b6 While this approach is very powerful, it comes at a detriment to the development process. pdb , which is the Python debugger, uses the sys.settrace API to provide information during debugging sesesions. Similarly, the coverage tool, which provides code-coverage information of Python programs, also uses this entrypoint. sys.settrace only allows for one trace function to be set, meaning no tooling that also uses this API can coexist with another. Therefore, along a codepath that uses the entrypoint in question, determining code coverage, or attempting to debug, is simply not possible. API \u00b6 The project implements the required functionalities in the tracing module, in the classes of Tracer and TraceBatchUpdate , which trace and collect instances into a DataFrame by the following schema: Column Meaning Type Null? Filename Relative path to file of traced instance from project root string Never ClassModule Module of class traced instance is in string When not in a class' scope Class Name of class traced instance is in string When not in a class' scope FunctionName Name of function traced instance is in string When not in a function's scope LineNo Line number traced instance occurs on uint Never Category Number identifying context traced instance appears in int Never VarName Name of traced instance string Never TypeModule Module of traced instance's type string When the type is builtin Type Name of traced instance's type string Never Category can take on 5 different values, which are contained in the TraceDataCategory enum class: LOCAL_VARIABLE , GLOBAL_VARIABLE , CLASS_MEMBER , FUNCTION_PARAMETER and FUNCTION_RETURN . @decorators.trace - Minimally Intrusive Tracing API \u00b6 The fetching process generates instances of this decorator function where applicable. Each invocation parses the config file from the root of the project, and executes the tracing process on the marked callable. After tracing has concluded, the accumulated DataFrame in the Tracer is serialised under pytypes/{project}/{test_case}/{func_name}-{hash(df)}.pytype . The hashing is performed to force tests that are executed in loops (e.g. by @pytest.mark.parametrize ) to not overwrite their predecessor's data, which could cause valuable information that would indicate union types, to be lost. If the traced test causes an uncaught exception, then a similarly named file with an .err suffix is generated containing the traceback. Additionally, if the benchmark_performance value has been set to true in pytypes.toml , then additional tracing will be performed that does not store any trace data, and again with logging enabled but with optimisations turned off. The runtimes for each execution are serialised next to the logged trace files. Tracer - Setting sys.settrace and Collecting Data \u00b6 The events generated by the trace function are caught in the Tracer class' _on_trace_is_called method after its start_trace method has been called. This ends when its end_trace method being called. This functionality has again been wrapped in its active_trace method, which can be used in Python's with statements. These methods are called from the @decorators.trace function, as documented in the previous section . The implementation backs-up any previously set trace function by reading from sys.gettrace , and sets its own using sys.settrace . This newly set trace function handles the call , line and return events, and ignores the exception and opcode events, as no relevant data can be gleamed from these. Each event is handled in its own appropriately named method, and the tracer combines the DataFrame s generated by BatchTraceUpdate . When tracing is halted, the DataFrame is deduplicated to remove redundant information and the old trace function is restored. During tracing, the values for TypeModule and Type are derived from the type function, which is passed to the Resolver to mirror components to Python's from x.y import z import style. BatchTraceUpdate - Simplifying and Batching Trace Updates \u00b6 Despite the events emitted by sys.settrace being disjunct, the operations that must be performed on the basis thereof are not. For example, the handling of the return event is a superset of that of the line event. Furthermore, the line event must perform update operations for both local and global variables. Also, the general functionality necessary to update the DataFrame is repetitive, especially with FileName , ClassModule , Class and FunctionName being identical for every trace call within the same function. The BatchTraceUpdate class was designed to solve these issues; the aforementioned repetitive data is passed to the constructor, to be reused in its methods. These methods form a builder-pattern style interface for each relevant category, allowing updates to be chained as each event requires. After all updates have been handled, a DataFrame can be produced that is to added to the otherwise accumulated trace data.","title":"Tracing"},{"location":"workflow/tracing/#functionality","text":"The tracing module contains classes that facilitate the tracing process behind a user-friendly API. Note that it does not offer a command for performing the tracing procedure, which should instead be done by executing the decorated tests of the project . The testing process can be highly customised on a per-project basis, and as such represents a high-effort low-reward coding investment on this project's end.","title":"Functionality"},{"location":"workflow/tracing/#foundations-principles","text":"","title":"Foundations &amp; Principles"},{"location":"workflow/tracing/#syssettrace","text":"sys.settrace is a Python function that allows a callable to be set that is invoked on every line of Python code that comes after it. This registered callable, henceforth refered to as the trace function is expected to have three arguments: frame : a representation of the current stack frame, containing references to further execution-related objects, such as: the previous frame visible globals variables that have been placed on the stack and more event : a string indicating the manner in which the current line of Python is handled. Relevant for us are: call: a callable was entered line: plain line of code that is about to be executed (NOTE: this means the line will executed in the next interpreter step, not when it is encountered by the trace function) return: a callable is about to return arg : a value that differs depending on the given event. Relevant for us are: call: arg is None. Retrieving the values of arguments is to be performed separately. line: arg is None. Retrieving the values of variables on this line is also to be performed separately. return: arg is the value that will be returned from the callable.","title":"sys.settrace"},{"location":"workflow/tracing/#effect-on-coverage-debugging-and-other-trace-related-tooling","text":"While this approach is very powerful, it comes at a detriment to the development process. pdb , which is the Python debugger, uses the sys.settrace API to provide information during debugging sesesions. Similarly, the coverage tool, which provides code-coverage information of Python programs, also uses this entrypoint. sys.settrace only allows for one trace function to be set, meaning no tooling that also uses this API can coexist with another. Therefore, along a codepath that uses the entrypoint in question, determining code coverage, or attempting to debug, is simply not possible.","title":"Effect on Coverage, Debugging and other trace-related Tooling"},{"location":"workflow/tracing/#api","text":"The project implements the required functionalities in the tracing module, in the classes of Tracer and TraceBatchUpdate , which trace and collect instances into a DataFrame by the following schema: Column Meaning Type Null? Filename Relative path to file of traced instance from project root string Never ClassModule Module of class traced instance is in string When not in a class' scope Class Name of class traced instance is in string When not in a class' scope FunctionName Name of function traced instance is in string When not in a function's scope LineNo Line number traced instance occurs on uint Never Category Number identifying context traced instance appears in int Never VarName Name of traced instance string Never TypeModule Module of traced instance's type string When the type is builtin Type Name of traced instance's type string Never Category can take on 5 different values, which are contained in the TraceDataCategory enum class: LOCAL_VARIABLE , GLOBAL_VARIABLE , CLASS_MEMBER , FUNCTION_PARAMETER and FUNCTION_RETURN .","title":"API"},{"location":"workflow/tracing/#decoratorstrace-minimally-intrusive-tracing-api","text":"The fetching process generates instances of this decorator function where applicable. Each invocation parses the config file from the root of the project, and executes the tracing process on the marked callable. After tracing has concluded, the accumulated DataFrame in the Tracer is serialised under pytypes/{project}/{test_case}/{func_name}-{hash(df)}.pytype . The hashing is performed to force tests that are executed in loops (e.g. by @pytest.mark.parametrize ) to not overwrite their predecessor's data, which could cause valuable information that would indicate union types, to be lost. If the traced test causes an uncaught exception, then a similarly named file with an .err suffix is generated containing the traceback. Additionally, if the benchmark_performance value has been set to true in pytypes.toml , then additional tracing will be performed that does not store any trace data, and again with logging enabled but with optimisations turned off. The runtimes for each execution are serialised next to the logged trace files.","title":"@decorators.trace - Minimally Intrusive Tracing API"},{"location":"workflow/tracing/#tracer-setting-syssettrace-and-collecting-data","text":"The events generated by the trace function are caught in the Tracer class' _on_trace_is_called method after its start_trace method has been called. This ends when its end_trace method being called. This functionality has again been wrapped in its active_trace method, which can be used in Python's with statements. These methods are called from the @decorators.trace function, as documented in the previous section . The implementation backs-up any previously set trace function by reading from sys.gettrace , and sets its own using sys.settrace . This newly set trace function handles the call , line and return events, and ignores the exception and opcode events, as no relevant data can be gleamed from these. Each event is handled in its own appropriately named method, and the tracer combines the DataFrame s generated by BatchTraceUpdate . When tracing is halted, the DataFrame is deduplicated to remove redundant information and the old trace function is restored. During tracing, the values for TypeModule and Type are derived from the type function, which is passed to the Resolver to mirror components to Python's from x.y import z import style.","title":"Tracer - Setting sys.settrace and Collecting Data"},{"location":"workflow/tracing/#batchtraceupdate-simplifying-and-batching-trace-updates","text":"Despite the events emitted by sys.settrace being disjunct, the operations that must be performed on the basis thereof are not. For example, the handling of the return event is a superset of that of the line event. Furthermore, the line event must perform update operations for both local and global variables. Also, the general functionality necessary to update the DataFrame is repetitive, especially with FileName , ClassModule , Class and FunctionName being identical for every trace call within the same function. The BatchTraceUpdate class was designed to solve these issues; the aforementioned repetitive data is passed to the constructor, to be reused in its methods. These methods form a builder-pattern style interface for each relevant category, allowing updates to be chained as each event requires. After all updates have been handled, a DataFrame can be produced that is to added to the otherwise accumulated trace data.","title":"BatchTraceUpdate - Simplifying and Batching Trace Updates"}]}